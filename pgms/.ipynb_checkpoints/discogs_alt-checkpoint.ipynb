{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.16 Safari/537.36\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## allows us to use reg expressions to search fields\n",
    "import re\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "# user agent\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "options = Options()\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random\n",
    "print(userAgent)\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "\n",
    "#driver = webdriver.Chrome(chrome_options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-6005edc26efb>:1: DeprecationWarning: use options instead of chrome_options\n",
      "  driver = webdriver.Chrome(chromedriver,chrome_options=options)\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(chromedriver,chrome_options=options)\n",
    "\n",
    "# update the page here to start where left off. Page is essentially the count as well\n",
    "pagestart=50\n",
    "pageend=100\n",
    "\n",
    "#_url = \"https://www.discogs.com/sell/list?sort=price%2Cdesc&limit=100&year1=1900&year2=1970&format=Vinyl&price=over40&genre=Jazz&currency=USD&style=Hard+Bop&page=\"\n",
    "# not just bebop\n",
    "_url = \"https://www.discogs.com/sell/list?sort=price%2Cdesc&limit=100&year1=1900&year2=1970&format=Vinyl&price=over40&genre=Jazz&currency=USD&page=\"\n",
    "url= _url + str(pagestart) + \"#more%3Dyear\" \n",
    "\n",
    "# starting url\n",
    "#driver.get(\"https://www.discogs.com/sell/list?sort=price%2Cdesc&limit=100&year1=1900&year2=1970&format=Vinyl&price=over40&genre=Jazz&currency=USD&style=Hard+Bop&page=3#more%3Dyear\")\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "with open('recorddata.csv', 'w',newline='') as csvfile:\n",
    "    file = csv.writer(csvfile)\n",
    "    # make headers\n",
    "    file.writerow(['Artist_Album', 'Label', 'Country', 'Format', 'Notes', 'Genre', 'Release_Date', 'Style', 'Rate_Haves_Wants', 'Media_Condition',\n",
    "                     'Sleeve_Condition', 'Seller_Rating', 'Recorded_at', 'Pressed_by', 'Price'])\n",
    "    count = pagestart #3\n",
    "    \n",
    "    while count < pageend: # 7:\n",
    "        \n",
    "        # find all links on album marketplace page and store in list\n",
    "\n",
    "        result_elements = '//a[contains(@href, \"/sell/item/\")]'\n",
    "\n",
    "        albums = []\n",
    "\n",
    "        albumdriver = driver.find_elements_by_xpath(result_elements)\n",
    "\n",
    "        for url in albumdriver:\n",
    "            albums.extend([url.get_attribute('href')])\n",
    "        \n",
    "        # get rid of duplicates\n",
    "\n",
    "        albumsclean = [album.split('?', 1)[0] for album in albums]\n",
    "        albumurls = set(albumsclean)\n",
    "\n",
    "        # scrape info from album page\n",
    "\n",
    "        for album in albumurls:\n",
    "            \n",
    "            driver.get(album)\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                artist_album = driver.find_element_by_xpath(\"//h1[contains(@id, 'profile_title')]\").text.strip()\n",
    "            except:\n",
    "                artist_album = np.nan\n",
    "        \n",
    "            ## cc: label\n",
    "            try:\n",
    "                rlabel = driver.find_element_by_xpath(\"//div[contains(text(), 'Label:')]/following-sibling::div\").text.strip()\n",
    "            except:\n",
    "                rlabel = np.nan\n",
    "                \n",
    "            ## cc: country\n",
    "            try:\n",
    "                country = driver.find_element_by_xpath(\"//div[contains(text(), 'Country:')]/following-sibling::div\").text.strip()\n",
    "            except:\n",
    "                country = np.nan    \n",
    "            \n",
    "            ## cc: format\n",
    "            try:\n",
    "                rformat = driver.find_element_by_xpath(\"//div[contains(text(), 'Format:')]/following-sibling::div\").text.strip()\n",
    "            except:\n",
    "                rformat = np.nan    \n",
    "                \n",
    "            ## cc: notes\n",
    "            try:\n",
    "                notes = driver.find_element_by_xpath(\"//h3[contains(text(), 'Notes')]/following-sibling::div\").text.strip()\n",
    "            except:\n",
    "                notes = np.nan    \n",
    "                        \n",
    "            \n",
    "            try:\n",
    "                genre = driver.find_element_by_xpath(\"//div[contains(text(), 'Genre:')]/following-sibling::div\").text.strip()\n",
    "            except:\n",
    "                genre = np.nan\n",
    "\n",
    "            try:\n",
    "                release_date = driver.find_element_by_xpath(\"//div[contains(text(), 'Released:')]/following-sibling::div\").text.strip()\n",
    "            except:\n",
    "                release_date = np.nan\n",
    "\n",
    "            try:\n",
    "                style = driver.find_element_by_xpath(\"//div[contains(text(), 'Style:')]/following-sibling::div\").text.strip()\n",
    "            except:\n",
    "                style = np.nan\n",
    "\n",
    "            try:\n",
    "                rate_haves_wants = driver.find_element_by_xpath(\"//a[contains(@class, 'button-blue')]/following-sibling::div\").text.strip()\n",
    "            except:\n",
    "                rate_haves_wants = np.nan\n",
    "\n",
    "            try:\n",
    "                m_condition = driver.find_element_by_xpath(\"//strong[contains(text(), 'Media:')]/following-sibling::span\").text.strip()\n",
    "            except:\n",
    "                m_condition = np.nan\n",
    "    \n",
    "            try:\n",
    "                sleeve = driver.find_element_by_xpath(\"//strong[contains(text(), 'Sleeve:')]\")\n",
    "                s_condition = sleeve.find_element_by_xpath('..').text.strip()\n",
    "            except:\n",
    "                s_condition = np.nan\n",
    "\n",
    "            try:\n",
    "                seller_rating = driver.find_element_by_xpath(\"//span[@class='star_rating']/following-sibling::strong\").text.strip()\n",
    "            except:\n",
    "                seller_rating = np.nan\n",
    "\n",
    "            # recorded at        \n",
    "            try:\n",
    "                recorded_at = driver.find_element_by_xpath(\"//span[contains(text(),'Recorded At')]/following-sibling::a\").text.strip()\n",
    "            except:\n",
    "                recorded_at = np.nan    \n",
    "                \n",
    "            # pressed at\n",
    "            try:\n",
    "                pressed_by = driver.find_element_by_xpath(\"//span[contains(text(),'Pressed By')]/following-sibling::a\").text.strip()\n",
    "            except:\n",
    "                pressed_by = np.nan    \n",
    "    \n",
    "\n",
    "            try:\n",
    "                price = soup.find(class_='price').text.strip()\n",
    "            except:\n",
    "                price = np.nan\n",
    "\n",
    "            observation = [artist_album, rlabel, country, rformat, notes, genre, release_date, style, rate_haves_wants, m_condition, \n",
    "                           s_condition, seller_rating, recorded_at, pressed_by, price]\n",
    "\n",
    "            file.writerow(observation)\n",
    "\n",
    "            time.sleep(.5+2*random.random())\n",
    "            \n",
    "        # go to next page in the marketplace\n",
    "        \n",
    "        #nextpage = \"https://www.discogs.com/sell/list?sort=listed%2Casc&currency=USD&limit=25&page=\" + str(count+1) + \"&format=Vinyl\"\n",
    "        #nextpage = \"https://www.discogs.com/sell/list?sort=price%2Cdesc&limit=100&year1=1900&year2=1970&format=Vinyl&price=over40&genre=Jazz&currency=USD&style=Hard+Bop&page=\" + str(count+1) + \"#more%3Dyear\"\n",
    "        nextpage = \"https://www.discogs.com/sell/list?sort=price%2Cdesc&limit=100&year1=1900&year2=1970&format=Vinyl&price=over40&genre=Jazz&currency=USD&page=\" + str(count+1)\n",
    "        driver.get(nextpage)\n",
    "        \n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
